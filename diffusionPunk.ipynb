{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAULAQqm727ehWaBgRDlgc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McRxnBCLdAZO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "d3LuY2S8dF53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 24\n",
        "CHANNELS_IMG = 4\n",
        "NUM_EPOCHS = 20\n",
        "TIME_STEPS = 200\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "3G__YLGcdTN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PunkDataset(Dataset):\n",
        "    def __init__(self, image_folder, transform=None):\n",
        "        self.transform = transform\n",
        "        self.image_folder = image_folder\n",
        "        files = os.listdir(image_folder)\n",
        "        self.n = len(files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_folder, str(idx) + '.png')\n",
        "        img = Image.open(img_path)\n",
        "        img = self.transform(img)\n",
        "        return img"
      ],
      "metadata": {
        "id": "L853sExqdaFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, in_channels=4, out_channels=4, features=[64, 128]):\n",
        "        super(DiffusionModel, self).__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        # Downsampling path\n",
        "        for feature in features:\n",
        "            self.downs.append(nn.Conv2d(in_channels, feature, 3, padding=1))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Upsampling path\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=4, stride=2, padding=1)\n",
        "            )\n",
        "\n",
        "        self.bottleneck = nn.Conv2d(features[-1], features[-1] * 2, 3, padding=1, stride=2)\n",
        "        self.final = nn.Conv2d(features[0] * 2, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = F.relu(down(x))\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        x = F.relu(self.bottleneck(x))\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx, up in enumerate(self.ups):\n",
        "            x = up(x)\n",
        "            skip = skip_connections[idx]\n",
        "            # Concatenate along channel dimension\n",
        "            concat_skip = torch.cat((skip, x), dim=1)\n",
        "            x = F.relu(concat_skip)\n",
        "\n",
        "        return self.final(x)"
      ],
      "metadata": {
        "id": "tK6e2OFddk_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5 for i in range(CHANNELS_IMG)], [0.5 for i in range(CHANNELS_IMG)])\n",
        "])\n",
        "dataset = PunkDataset('drive/MyDrive/data', transform=transform)\n",
        "trainloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "model = DiffusionModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "aFJziEmKdlHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    for images in trainloader:\n",
        "        images = images.to(device)\n",
        "        # Uniformly sample timesteps t for each element in the batch\n",
        "        t = torch.randint(1, TIME_STEPS + 1, (images.shape[0],), device=device)\n",
        "\n",
        "        # Sample standard gaussian noise\n",
        "        epsilon = torch.randn_like(images).to(device)\n",
        "\n",
        "        noise_images =\n",
        "        # Future training steps:\n",
        "        # 1. Add noise to images based on t\n",
        "        # 2. Model predicts epsilon\n",
        "        # 3. Compute loss and optimize"
      ],
      "metadata": {
        "id": "NnhgOzhQgt4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}