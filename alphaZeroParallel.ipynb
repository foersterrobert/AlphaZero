{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import trange\n",
    "from mcts import MCTS\n",
    "\n",
    "from games import ConnectFour, TicTacToe\n",
    "from models import ResNet\n",
    "from utils import KaggleAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, player, prior, game, args, parent=None, action_taken=None):\n",
    "        self.state = state\n",
    "        self.player = player\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.total_value = 0\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.action_taken = action_taken\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "\n",
    "    def expand(self, action_probs):\n",
    "        for a, prob in enumerate(action_probs):\n",
    "            if prob != 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.drop_piece(child_state, a, self.player)\n",
    "                child = Node(\n",
    "                    child_state,\n",
    "                    self.game.get_opponent_player(self.player),\n",
    "                    prob,\n",
    "                    self.game,\n",
    "                    self.args,\n",
    "                    parent=self,\n",
    "                    action_taken=a,\n",
    "                )\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.total_value += value\n",
    "        self.visit_count += 1\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(self.game.get_opponent_value(value))\n",
    "\n",
    "    def is_expandable(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select_child(self):\n",
    "        best_score = -np.inf\n",
    "        best_child = None\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb_score = self.get_ucb_score(child)\n",
    "            if ucb_score > best_score:\n",
    "                best_score = ucb_score\n",
    "                best_child = child\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb_score(self, child):\n",
    "        prior_score = self.args['c_puct'] * child.prior * math.sqrt(self.visit_count) / (1 + child.visit_count)\n",
    "        if child.visit_count == 0:\n",
    "            return prior_score\n",
    "        return prior_score - (child.total_value / child.visit_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayGame:\n",
    "    def __init__(self, game):\n",
    "        self.game_memory = []\n",
    "        self.player = 1\n",
    "        self.state = game.get_initial_state()\n",
    "        self.root = None\n",
    "        self.node = None\n",
    "        self.encoded_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.model, self.game, self.args)\n",
    "\n",
    "    def self_play(self, group_size=10):\n",
    "        self_play_games = [SelfPlayGame(self.game) for _ in range(group_size)]\n",
    "        self_play_memory = []\n",
    "\n",
    "        while len(self_play_games) > 0:\n",
    "            del_list = []\n",
    "\n",
    "            for game in self_play_games:\n",
    "                game.root = Node(self.game.get_canonical_state(game.state, game.player), 1, prior=0, game=self.game, args=self.args)\n",
    "\n",
    "            for simulation in range(self.args['num_simulation_games']):\n",
    "                for game in self_play_games:\n",
    "                    game.encoded_state = None\n",
    "                    node = game.root\n",
    "\n",
    "                    while node.is_expandable():\n",
    "                        node = node.select_child()\n",
    "\n",
    "                    is_terminal, value = self.game.check_terminal_and_value(node.state, node.action_taken)\n",
    "                    value = self.game.get_opponent_value(value)\n",
    "\n",
    "                    if is_terminal:\n",
    "                        node.backpropagate(value)\n",
    "\n",
    "                    else:\n",
    "                        canonical_state = self.game.get_canonical_state(node.state, node.player)\n",
    "                        game.encoded_state = self.game.get_encoded_state(canonical_state)\n",
    "                        game.node = node\n",
    "\n",
    "                self_play_games_predict = [game for game in self_play_games if game.encoded_state is not None]\n",
    "                if len(self_play_games_predict) > 0:\n",
    "                    predict_states = [game.encoded_state for game in self_play_games_predict]\n",
    "                    if len(predict_states) > 1:\n",
    "                        predict_states = np.stack(predict_states).reshape(-1, 3, 6, 7)\n",
    "                    else:\n",
    "                        predict_states = np.array(predict_states).reshape(1, 3, 6, 7)\n",
    "                    predict_states = torch.from_numpy(predict_states).float().to(self.device)\n",
    "                    action_probs, values = self.model(predict_states)\n",
    "                    action_probs = torch.softmax(action_probs, dim=1)\n",
    "                    action_probs = action_probs.cpu().detach().numpy()\n",
    "                    values = values.cpu().detach().numpy()\n",
    "\n",
    "                for i, game in enumerate(self_play_games_predict):\n",
    "                    action_probs_game, value_game = action_probs[i], values[i]\n",
    "                    valid_moves = self.game.get_valid_locations(game.node.state)\n",
    "                    action_probs_game = action_probs_game * valid_moves\n",
    "                    action_probs_game = action_probs_game / np.sum(action_probs_game)\n",
    "                    game.node.expand(action_probs_game)\n",
    "                    game.node.backpropagate(value_game)\n",
    "\n",
    "            for game in self_play_games:\n",
    "                action_probs = [0] * self.game.action_size\n",
    "                for child in game.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                game.game_memory.append((game.root.state, game.player, action_probs))\n",
    "\n",
    "                visit_counts = [child.visit_count for child in game.root.children]\n",
    "                actions = [child.action_taken for child in game.root.children]\n",
    "                if self.args['temperature'] == 0:\n",
    "                    action = actions[np.argmax(visit_counts)]\n",
    "                elif self.args['temperature'] == float('inf'):\n",
    "                    action = np.random.choice(actions)\n",
    "                else:\n",
    "                    visit_count_distribution = np.array(visit_counts) ** (1 / self.args['temperature'])\n",
    "                    visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "                    action = np.random.choice(actions, p=visit_count_distribution)\n",
    "\n",
    "                game.state = self.game.drop_piece(game.state, action, game.player)\n",
    "\n",
    "                is_terminal, reward = self.game.check_terminal_and_value(game.state, action)\n",
    "                if is_terminal:\n",
    "                    return_memory = []\n",
    "                    for hist_state, hist_player, hist_action_probs in game.game_memory:\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_state), hist_action_probs, reward * ((-1) ** (hist_player != game.player))\n",
    "                        ))\n",
    "                        if self.args['augment']:\n",
    "                            return_memory.append((\n",
    "                                self.game.get_encoded_state(self.game.get_augmented_state(hist_state)), np.flip(hist_action_probs), reward * ((-1) ** (hist_player != game.player))\n",
    "                            ))\n",
    "                    self_play_memory.extend(return_memory)\n",
    "                    del_list.append(game)\n",
    "\n",
    "                else:\n",
    "                    game.player = self.game.get_opponent_player(game.player)\n",
    "\n",
    "            for game in del_list:\n",
    "                self_play_games.remove(game)\n",
    "                \n",
    "        return self_play_memory\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory) -1, self.args['batch_size']):\n",
    "            state, policy, value = list(zip(*memory[batchIdx:min(len(memory) -1, batchIdx + self.args['batch_size'])]))\n",
    "            state, policy, value = np.array(state), np.array(policy), np.array(value).reshape(-1, 1)\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            policy = torch.tensor(policy, dtype=torch.float32).to(self.device)\n",
    "            value = torch.tensor(value, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "            loss_policy = F.cross_entropy(out_policy, policy) \n",
    "            loss_value = F.mse_loss(out_value, value)\n",
    "            loss = loss_policy + loss_value\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            print(f\"iteration: {iteration}\")\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            for train_game in trange(5, desc=\"train_game\"):\n",
    "                memory += self.self_play(200)\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs'], desc=\"epoch\"):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f\"Models/{self.game}/model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"Models/{self.game}/optimizer_{iteration}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "GAME = 'ConnectFour'\n",
    "LOAD = True\n",
    "\n",
    "if GAME == 'ConnectFour':\n",
    "    args = {\n",
    "        'num_iterations': 48,             # number of highest level iterations\n",
    "        'num_train_games': 500,           # number of self-play games to play within each iteration\n",
    "        'num_simulation_games': 600,      # number of mcts simulations when selecting a move within self-play\n",
    "        'num_epochs': 4,                  # number of epochs for training on self-play data for each iteration\n",
    "        'batch_size': 128,                # batch size for training\n",
    "        'temperature': 1,                 # temperature for the softmax selection of moves\n",
    "        'c_puct': 2,                      # the value of the constant policy\n",
    "        'augment': True,                  # whether to augment the training data with flipped states\n",
    "    }\n",
    "\n",
    "    game = ConnectFour()\n",
    "    model = ResNet(9, game).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    if LOAD:\n",
    "        model.load_state_dict(torch.load(f'Models/{game}/model.pt', map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(f'Models/{game}/optimizer.pt', map_location=device))\n",
    "\n",
    "elif GAME == 'TicTacToe':\n",
    "        args = {\n",
    "            'num_iterations': 8,              # number of highest level iterations\n",
    "            'num_train_games': 500,           # number of self-play games to play within each iteration\n",
    "            'num_simulation_games': 60,       # number of mcts simulations when selecting a move within self-play\n",
    "            'num_epochs': 4,                  # number of epochs for training on self-play data for each iteration\n",
    "            'batch_size': 64,                 # batch size for training\n",
    "            'temperature': 1,                 # temperature for the softmax selection of moves\n",
    "            'c_puct': 2,                      # the value of the constant policy\n",
    "            'augment': False,                 # whether to augment the training data with flipped states\n",
    "        }\n",
    "\n",
    "        game = TicTacToe()\n",
    "        model = ResNet(4, game).to(device)\n",
    "        optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        if LOAD:\n",
    "            model.load_state_dict(torch.load(f'Models/{game}/model.pt', map_location=device))\n",
    "            optimizer.load_state_dict(torch.load(f'Models/{game}/optimizer.pt', map_location=device))\n",
    "\n",
    "trainer = Trainer(model, optimizer, game, args)\n",
    "trainer.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
